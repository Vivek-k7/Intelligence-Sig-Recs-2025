{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":31154,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np \nimport pandas as pd\nimport torch\nfrom datasets import load_dataset\n\nnewsqa = load_dataset(\"StellarMilk/newsqa\")\nprint(newsqa)\n\ntrain = newsqa[\"train\"]","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-10-20T11:39:09.365859Z","iopub.execute_input":"2025-10-20T11:39:09.366671Z","iopub.status.idle":"2025-10-20T11:39:11.345120Z","shell.execute_reply.started":"2025-10-20T11:39:09.366635Z","shell.execute_reply":"2025-10-20T11:39:11.344446Z"}},"outputs":[{"name":"stdout","text":"DatasetDict({\n    train: Dataset({\n        features: ['paragraph', 'questions', 'answers', 'questions_answers'],\n        num_rows: 10327\n    })\n    validation: Dataset({\n        features: ['paragraph', 'questions', 'answers', 'questions_answers'],\n        num_rows: 574\n    })\n    test: Dataset({\n        features: ['paragraph', 'questions', 'answers', 'questions_answers'],\n        num_rows: 574\n    })\n})\n","output_type":"stream"}],"execution_count":24},{"cell_type":"code","source":"df = train.to_pandas()\ndf.head()\ndf.isnull().sum()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-20T11:39:11.346762Z","iopub.execute_input":"2025-10-20T11:39:11.347032Z","iopub.status.idle":"2025-10-20T11:39:11.467873Z","shell.execute_reply.started":"2025-10-20T11:39:11.347016Z","shell.execute_reply":"2025-10-20T11:39:11.467186Z"}},"outputs":[{"execution_count":25,"output_type":"execute_result","data":{"text/plain":"paragraph            0\nquestions            0\nanswers              0\nquestions_answers    0\ndtype: int64"},"metadata":{}}],"execution_count":25},{"cell_type":"markdown","source":"## Data Preprocessing(cleaning)","metadata":{}},{"cell_type":"markdown","source":"#### we use a contraction map to map contractions to their respective full forms: to reduce extra single/double letter tokens which would, by themselves, have no meaning\n#### convert all text to lower case; extract only letters as we only care about the words and their general- static meaning; also replace all whitespace characters with a single space","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport re\n\nCONTRACTION_MAP = {\n    \"ain't\": \"am not\", \"aren't\": \"are not\", \"can't\": \"cannot\",\n    \"can't've\": \"cannot have\", \"'cause\": \"because\", \"could've\": \"could have\",\n    \"couldn't\": \"could not\", \"couldn't've\": \"could not have\", \"didn't\": \"did not\",\n    \"doesn't\": \"does not\", \"don't\": \"do not\", \"hadn't\": \"had not\",\n    \"hadn't've\": \"had not have\", \"hasn't\": \"has not\", \"haven't\": \"have not\",\n    \"he'd\": \"he would\", \"he'd've\": \"he would have\", \"he'll\": \"he will\",\n    \"he'll've\": \"he will have\", \"he's\": \"he is\", \"how'd\": \"how did\",\n    \"how'd'y\": \"how do you\", \"how'll\": \"how will\", \"how's\": \"how is\",\n    \"I'd\": \"I would\", \"I'd've\": \"I would have\", \"I'll\": \"I will\",\n    \"I'll've\": \"I will have\", \"I'm\": \"I am\", \"I've\": \"I have\",\n    \"isn't\": \"is not\", \"it'd\": \"it would\", \"it'd've\": \"it would have\",\n    \"it'll\": \"it will\", \"it'll've\": \"it will have\", \"it's\": \"it is\",\n    \"let's\": \"let us\", \"ma'am\": \"madam\", \"mayn't\": \"may not\",\n    \"might've\": \"might have\", \"mightn't\": \"might not\", \"mightn't've\": \"might not have\",\n    \"must've\": \"must have\", \"mustn't\": \"must not\", \"mustn't've\": \"must not have\",\n    \"needn't\": \"need not\", \"needn't've\": \"need not have\", \"o'clock\": \"of the clock\",\n    \"oughtn't\": \"ought not\", \"oughtn't've\": \"ought not have\", \"shan't\": \"shall not\",\n    \"sha'n't\": \"shall not\", \"shan't've\": \"shall not have\", \"she'd\": \"she would\",\n    \"she'd've\": \"she would have\", \"she'll\": \"she will\", \"she'll've\": \"she will have\",\n    \"she's\": \"she is\", \"should've\": \"should have\", \"shouldn't\": \"should not\",\n    \"shouldn't've\": \"should not have\", \"so've\": \"so have\", \"so's\": \"so is\",\n    \"that'd\": \"that would\", \"that'd've\": \"that would have\", \"that's\": \"that is\",\n    \"there'd\": \"there would\", \"there'd've\": \"there would have\", \"there's\": \"there is\",\n    \"they'd\": \"they would\", \"they'd've\": \"they would have\", \"they'll\": \"they will\",\n    \"they'll've\": \"they will have\", \"they're\": \"they are\", \"they've\": \"they have\",\n    \"to've\": \"to have\", \"wasn't\": \"was not\", \"we'd\": \"we would\",\n    \"we'd've\": \"we would have\", \"we'll\": \"we will\", \"we'll've\": \"we will have\",\n    \"we're\": \"we are\", \"we've\": \"we have\", \"weren't\": \"were not\",\n    \"what'll\": \"what will\", \"what'll've\": \"what will have\", \"what're\": \"what are\",\n    \"what's\": \"what is\", \"what've\": \"what have\", \"when's\": \"when is\",\n    \"when've\": \"when have\", \"where'd\": \"where did\", \"where's\": \"where is\",\n    \"where've\": \"where have\", \"who'll\": \"who will\", \"who'll've\": \"who will have\",\n    \"who's\": \"who is\", \"who've\": \"who have\", \"why's\": \"why is\",\n    \"why've\": \"why have\", \"will've\": \"will have\", \"won't\": \"will not\",\n    \"won't've\": \"will not have\", \"would've\": \"would have\", \"wouldn't\": \"would not\",\n    \"wouldn't've\": \"would not have\", \"y'all\": \"you all\", \"y'all'd\": \"you all would\",\n    \"y'all'd've\": \"you all would have\", \"y'all're\": \"you all are\", \"y'all've\": \"you all have\",\n    \"you'd\": \"you would\", \"you'd've\": \"you would have\", \"you'll\": \"you will\",\n    \"you'll've\": \"you all have\", \"you're\": \"you are\", \"you've\": \"you have\"\n}\n# above is generated by gpt\n# Regex pattern to find contractions quickly\nCONTRACTIONS_RE = re.compile('({})'.format('|'.join(re.escape(key) for key in CONTRACTION_MAP.keys())), re.IGNORECASE)\n\ndef expand_contractions(text, CONTRACTION_MAP):\n    def replace(match):\n        try:\n            return CONTRACTION_MAP[match.group(0).lower()]\n        except KeyError:\n            return\n    return CONTRACTIONS_RE.sub(replace, text)\n\n#above was done just to reduce number of tokens; as certain letters like 's', 'll' etc would also be counted.\n\ndef clean_text(text):\n    text = expand_contractions(text)\n    text = text.lower()\n    text = re.sub(r'[^a-z\\s]', '', text)\n    text = re.sub(r'\\s+', ' ', text).strip()\n\n    return text\n\ndf['paragraph'] = df['paragraph'].apply(clean_text)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-20T11:39:11.468624Z","iopub.execute_input":"2025-10-20T11:39:11.468932Z","iopub.status.idle":"2025-10-20T11:39:56.825189Z","shell.execute_reply.started":"2025-10-20T11:39:11.468914Z","shell.execute_reply":"2025-10-20T11:39:56.824628Z"}},"outputs":[],"execution_count":26},{"cell_type":"markdown","source":"## Model initialization","metadata":{}},{"cell_type":"code","source":"import gensim\n\nmodel = gensim.models.Word2Vec(\n    window = 10,                               # Radius of the sliding window\n    min_count = 2,                             # ignores all words with less than min_count frequency\n    vector_size = 200,                         # vector embeddings size\n    epochs = 10           \n)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-20T11:39:56.825968Z","iopub.execute_input":"2025-10-20T11:39:56.826224Z","iopub.status.idle":"2025-10-20T11:39:56.835013Z","shell.execute_reply.started":"2025-10-20T11:39:56.826207Z","shell.execute_reply":"2025-10-20T11:39:56.834327Z"}},"outputs":[],"execution_count":27},{"cell_type":"markdown","source":"### Creating the corpus","metadata":{}},{"cell_type":"code","source":"corpus = [i.split() for i in list(df['paragraph'])]         # list of sentences which has been split","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-20T11:51:03.318202Z","iopub.execute_input":"2025-10-20T11:51:03.318907Z","iopub.status.idle":"2025-10-20T11:51:03.718240Z","shell.execute_reply.started":"2025-10-20T11:51:03.318883Z","shell.execute_reply":"2025-10-20T11:51:03.717694Z"}},"outputs":[],"execution_count":36},{"cell_type":"markdown","source":"### Building the Vocab","metadata":{}},{"cell_type":"code","source":"model.build_vocab(corpus)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-20T11:40:13.318130Z","iopub.execute_input":"2025-10-20T11:40:13.318845Z","iopub.status.idle":"2025-10-20T11:40:14.875351Z","shell.execute_reply.started":"2025-10-20T11:40:13.318820Z","shell.execute_reply":"2025-10-20T11:40:14.874574Z"}},"outputs":[],"execution_count":30},{"cell_type":"markdown","source":"### Training the model","metadata":{}},{"cell_type":"code","source":"model.train(corpus, total_examples = model.corpus_count, epochs = model.epochs)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-20T11:40:15.524936Z","iopub.execute_input":"2025-10-20T11:40:15.525475Z","iopub.status.idle":"2025-10-20T11:41:07.466993Z","shell.execute_reply.started":"2025-10-20T11:40:15.525451Z","shell.execute_reply":"2025-10-20T11:41:07.466362Z"}},"outputs":[{"execution_count":31,"output_type":"execute_result","data":{"text/plain":"(39736239, 51318650)"},"metadata":{}}],"execution_count":31},{"cell_type":"markdown","source":"## Example to show the use of this model:\n#### we can find the nearest words to a particular word in terms of its meaning learnt by training the model; this measure of how near a word is to a given word is calculated by cosine similarity between the given word's embeddings and other words' embeddings","metadata":{}},{"cell_type":"code","source":"model.wv.most_similar('apple')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-20T11:55:25.642134Z","iopub.execute_input":"2025-10-20T11:55:25.642439Z","iopub.status.idle":"2025-10-20T11:55:25.651266Z","shell.execute_reply.started":"2025-10-20T11:55:25.642417Z","shell.execute_reply":"2025-10-20T11:55:25.650531Z"}},"outputs":[{"execution_count":40,"output_type":"execute_result","data":{"text/plain":"[('apples', 0.8100547194480896),\n ('app', 0.7732735872268677),\n ('microsoft', 0.7679699659347534),\n ('google', 0.7653537392616272),\n ('iphone', 0.7572395205497742),\n ('samsung', 0.7503407597541809),\n ('android', 0.7503293752670288),\n ('iphones', 0.7369444966316223),\n ('apps', 0.7294312119483948),\n ('software', 0.7188068628311157)]"},"metadata":{}}],"execution_count":40}]}