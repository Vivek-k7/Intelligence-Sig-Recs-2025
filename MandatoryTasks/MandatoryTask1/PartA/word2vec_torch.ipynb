{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "e3d78652",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['paragraph', 'questions', 'answers', 'questions_answers'],\n",
      "        num_rows: 10327\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['paragraph', 'questions', 'answers', 'questions_answers'],\n",
      "        num_rows: 574\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['paragraph', 'questions', 'answers', 'questions_answers'],\n",
      "        num_rows: 574\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "import numpy as np \n",
    "import pandas as pd\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "\n",
    "newsqa = load_dataset(\"StellarMilk/newsqa\")\n",
    "print(newsqa)\n",
    "\n",
    "train = newsqa[\"train\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "679bf7d7",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "7d5b7f51",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "paragraph            0\n",
       "questions            0\n",
       "answers              0\n",
       "questions_answers    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = train.to_pandas()\n",
    "df.head()\n",
    "df.isnull().sum() "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86bb4481",
   "metadata": {},
   "source": [
    "#### no null rows"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ebd9d63",
   "metadata": {},
   "source": [
    "#### We use a contraction map to map popular contractions to their full forms\n",
    "#### Then convert the text to lower case; extract only letters from it as its enough for word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "e1897925",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "CONTRACTION_MAP = {\n",
    "    \"ain't\": \"am not\", \"aren't\": \"are not\", \"can't\": \"cannot\",\n",
    "    \"can't've\": \"cannot have\", \"'cause\": \"because\", \"could've\": \"could have\",\n",
    "    \"couldn't\": \"could not\", \"couldn't've\": \"could not have\", \"didn't\": \"did not\",\n",
    "    \"doesn't\": \"does not\", \"don't\": \"do not\", \"hadn't\": \"had not\",\n",
    "    \"hadn't've\": \"had not have\", \"hasn't\": \"has not\", \"haven't\": \"have not\",\n",
    "    \"he'd\": \"he would\", \"he'd've\": \"he would have\", \"he'll\": \"he will\",\n",
    "    \"he'll've\": \"he will have\", \"he's\": \"he is\", \"how'd\": \"how did\",\n",
    "    \"how'd'y\": \"how do you\", \"how'll\": \"how will\", \"how's\": \"how is\",\n",
    "    \"I'd\": \"I would\", \"I'd've\": \"I would have\", \"I'll\": \"I will\",\n",
    "    \"I'll've\": \"I will have\", \"I'm\": \"I am\", \"I've\": \"I have\",\n",
    "    \"isn't\": \"is not\", \"it'd\": \"it would\", \"it'd've\": \"it would have\",\n",
    "    \"it'll\": \"it will\", \"it'll've\": \"it will have\", \"it's\": \"it is\",\n",
    "    \"let's\": \"let us\", \"ma'am\": \"madam\", \"mayn't\": \"may not\",\n",
    "    \"might've\": \"might have\", \"mightn't\": \"might not\", \"mightn't've\": \"might not have\",\n",
    "    \"must've\": \"must have\", \"mustn't\": \"must not\", \"mustn't've\": \"must not have\",\n",
    "    \"needn't\": \"need not\", \"needn't've\": \"need not have\", \"o'clock\": \"of the clock\",\n",
    "    \"oughtn't\": \"ought not\", \"oughtn't've\": \"ought not have\", \"shan't\": \"shall not\",\n",
    "    \"sha'n't\": \"shall not\", \"shan't've\": \"shall not have\", \"she'd\": \"she would\",\n",
    "    \"she'd've\": \"she would have\", \"she'll\": \"she will\", \"she'll've\": \"she will have\",\n",
    "    \"she's\": \"she is\", \"should've\": \"should have\", \"shouldn't\": \"should not\",\n",
    "    \"shouldn't've\": \"should not have\", \"so've\": \"so have\", \"so's\": \"so is\",\n",
    "    \"that'd\": \"that would\", \"that'd've\": \"that would have\", \"that's\": \"that is\",\n",
    "    \"there'd\": \"there would\", \"there'd've\": \"there would have\", \"there's\": \"there is\",\n",
    "    \"they'd\": \"they would\", \"they'd've\": \"they would have\", \"they'll\": \"they will\",\n",
    "    \"they'll've\": \"they will have\", \"they're\": \"they are\", \"they've\": \"they have\",\n",
    "    \"to've\": \"to have\", \"wasn't\": \"was not\", \"we'd\": \"we would\",\n",
    "    \"we'd've\": \"we would have\", \"we'll\": \"we will\", \"we'll've\": \"we will have\",\n",
    "    \"we're\": \"we are\", \"we've\": \"we have\", \"weren't\": \"were not\",\n",
    "    \"what'll\": \"what will\", \"what'll've\": \"what will have\", \"what're\": \"what are\",\n",
    "    \"what's\": \"what is\", \"what've\": \"what have\", \"when's\": \"when is\",\n",
    "    \"when've\": \"when have\", \"where'd\": \"where did\", \"where's\": \"where is\",\n",
    "    \"where've\": \"where have\", \"who'll\": \"who will\", \"who'll've\": \"who will have\",\n",
    "    \"who's\": \"who is\", \"who've\": \"who have\", \"why's\": \"why is\",\n",
    "    \"why've\": \"why have\", \"will've\": \"will have\", \"won't\": \"will not\",\n",
    "    \"won't've\": \"will not have\", \"would've\": \"would have\", \"wouldn't\": \"would not\",\n",
    "    \"wouldn't've\": \"would not have\", \"y'all\": \"you all\", \"y'all'd\": \"you all would\",\n",
    "    \"y'all'd've\": \"you all would have\", \"y'all're\": \"you all are\", \"y'all've\": \"you all have\",\n",
    "    \"you'd\": \"you would\", \"you'd've\": \"you would have\", \"you'll\": \"you will\",\n",
    "    \"you'll've\": \"you all have\", \"you're\": \"you are\", \"you've\": \"you have\"\n",
    "}\n",
    "\n",
    "# above is generated by gpt\n",
    "# Regex pattern to find contractions quickly\n",
    "CONTRACTIONS_RE = re.compile('({})'.format('|'.join(re.escape(key) for key in CONTRACTION_MAP.keys())), re.IGNORECASE)\n",
    "\n",
    "def expand_contractions(text, contraction_mapping=CONTRACTION_MAP):\n",
    "    def replace(match):\n",
    "        # Get the matched contraction, convert to lowercase, and look up in map\n",
    "        try:\n",
    "            return contraction_mapping[match.group(0).lower()]\n",
    "        except KeyError:\n",
    "            return\n",
    "    # Apply the replacement using the regex pattern\n",
    "    return CONTRACTIONS_RE.sub(replace, text)\n",
    "\n",
    "#above was done just to reduce number of tokens; as certain letters like 's', 'll' etc would also be counted.\n",
    "\n",
    "def clean_text(text):\n",
    "    text = expand_contractions(text)\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'[^a-z\\s]', '', text)\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "\n",
    "    return text\n",
    "\n",
    "df['questions_answers'] = df['questions_answers'].apply(clean_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1635e71",
   "metadata": {},
   "source": [
    "# Vocabulary Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "28f69ea7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "text = \" \".join(df['questions_answers'].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "41dc3885",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "34041"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens = list(set(text.split()))             # first to set then to list as set narrows all the tokens from the corpus to only unique tokens\n",
    "len(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2106308f",
   "metadata": {},
   "source": [
    "# TF-IDF vectorization\n",
    "### Formula: TF * IDF where,\n",
    "#### TF (token, doc) = no. of times token occurs in that particular document / total no. of tokens in that document\n",
    "#### IDF (token) = log (Total no. of documents / no. of documents that contain that token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "830c8d61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF Matrix Shape: (10327, 34041)\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "N = len(df) \n",
    "\n",
    "# This is for counting how many documents each token appears in at least once.\n",
    "doc_freq = defaultdict(int)\n",
    "\n",
    "for doc_text in df['questions_answers']:\n",
    "    doc_text = re.sub(r'[^a-z\\s]', '', doc_text.lower())\n",
    "    doc_text = re.sub(r'\\s+', ' ', doc_text).strip()\n",
    "    for token in set(doc_text.split()):\n",
    "        doc_freq[token] += 1                       # takes all unique tokens in the doc and updates the frequency of all unique tokens in that document by 1\n",
    "\n",
    "# calculate the final IDF score for each token\n",
    "idf_scores = {token: np.log(N / (doc_freq[token] + 1)) for token in tokens}\n",
    "\n",
    "tfidf_matrix = np.zeros((len(df), len(tokens)))\n",
    "\n",
    "for doc_idx, i in enumerate(df['questions_answers']):\n",
    "    doc = \"\".join(i)\n",
    "    doc = doc.lower()\n",
    "    doc = re.sub(r'[^a-z\\s]', '', doc)\n",
    "    doc = re.sub(r'\\s+', ' ', doc).strip()\n",
    "\n",
    "    doc_words = doc.split()                    # all the tokens in the doc\n",
    "    n_doc = len(doc_words)                     # total no. of words/tokens in the document\n",
    "\n",
    "    # Avoid division by zero if a document is empty after processing()\n",
    "    if n_doc == 0:\n",
    "        continue\n",
    "        \n",
    "    for token_idx, j in enumerate(tokens):\n",
    "        # TF calculation\n",
    "        n_j = doc_words.count(j)               # no. of times token/word 'j' occurs in that particular document\n",
    "        tf = n_j / n_doc                       # TF calculation\n",
    "        idf = idf_scores[j]                    # Extracting idf which was pre computed\n",
    "        \n",
    "        tfidf_matrix[doc_idx, token_idx] = tf * idf\n",
    "\n",
    "print(\"TF-IDF Matrix Shape:\", tfidf_matrix.shape)   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7fdcedb",
   "metadata": {},
   "source": [
    "# Example:\n",
    "### finding cosine similarity between two similar words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "454e20e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(0.2578297499808587)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ar1 = np.array(tfidf_matrix[:, tokens.index('iphone')])\n",
    "ar2 = np.array(tfidf_matrix[:, tokens.index('apple')])\n",
    "dot_product = ar1@ar2\n",
    "norm_ar1 = np.linalg.norm(ar1)\n",
    "norm_ar2 = np.linalg.norm(ar2)\n",
    "if norm_ar1 > 0 and norm_ar2 > 0:\n",
    "    cosine_sim = dot_product / (norm_ar1 * norm_ar2)\n",
    "else:\n",
    "    cosine_sim = 0.0 \n",
    "cosine_sim\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19618a5d",
   "metadata": {},
   "source": [
    "# Word2Vec, CBOW torch implementation:\n",
    "### CBOW- Continuous Bag Of Words- uses a sliding window across the corpus; takes centre word as target and passes vocab indices of surrounding words(radius= r) and passes through embedding layer to get vector embeddings of those words then to an output layer which gives a one hot enc to extract centre word"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d9d6079",
   "metadata": {},
   "source": [
    "### Model Architecture:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "4cef61dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class W2V(nn.Module):\n",
    "    def __init__(self, vocab_size, emb_size):\n",
    "        super().__init__()\n",
    "        self.embed = nn.Embedding(num_embeddings = vocab_size, embedding_dim = emb_size)\n",
    "        self.out = nn.Linear(emb_size, vocab_size)\n",
    "    \n",
    "    def forward(self, indices):\n",
    "        embs = self.embed(indices)\n",
    "        avg = embs.mean(axis = 1)            # mean across the surrounding words\n",
    "        return self.out(avg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "5a89daf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "62616cd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = len(tokens)\n",
    "emb_size = 200\n",
    "vocab_size+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "20b45c25",
   "metadata": {},
   "outputs": [],
   "source": [
    "idx2val = {key: val for key, val in enumerate(tokens)} #as the names suggest\n",
    "val2idx = {val: key for key, val in enumerate(tokens)}\n",
    "idx2val[vocab_size-1] = ''\n",
    "val2idx[''] = vocab_size - 1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "41be3e52",
   "metadata": {},
   "outputs": [],
   "source": [
    "context_size = 10\n",
    "window_rad = (context_size - 1) // 2  \n",
    "\n",
    "pad_index = val2idx['']               # padding token\n",
    "\n",
    "contexts_data = []\n",
    "targets = []\n",
    "\n",
    "for doc in df['questions_answers']:\n",
    "    doc_tokens = doc.split()\n",
    "    \n",
    "    padded_toks = [''] * window_rad + doc_tokens + [''] * window_rad   #padding added this way to handle case when target word is at the start(in first radius) or end(last radius)\n",
    "    #Slide the window over the doc_tokens\n",
    "    for i in range(len(doc_tokens)):\n",
    "        target_word = doc_tokens[i]   #target\n",
    "        \n",
    "        center_position = i + window_rad            # where centre of the padded_toks represent the target\n",
    "        \n",
    "        words_before = padded_toks[center_position - window_rad : center_position]\n",
    "        words_after = padded_toks[center_position + 1 : center_position + 1 + window_rad]\n",
    "        \n",
    "        context_words = words_before + words_after\n",
    "        \n",
    "        try:\n",
    "            target_index = val2idx[target_word]\n",
    "            context_indices = [val2idx[word] for word in context_words]\n",
    "            targets.append(target_index)\n",
    "            contexts_data.append(context_indices)\n",
    "\n",
    "        except KeyError as e:        # if word not present in vocab\n",
    "            pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "c420382f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([[34041, 34041, 34041, 34041, 16818, 24139, 30891, 7454],\n",
       "  [34041, 34041, 34041, 16042, 24139, 30891, 7454, 11648],\n",
       "  [34041, 34041, 16042, 16818, 30891, 7454, 11648, 26755],\n",
       "  [34041, 16042, 16818, 24139, 7454, 11648, 26755, 16042],\n",
       "  [16042, 16818, 24139, 30891, 11648, 26755, 16042, 25135]],\n",
       " [16042, 16818, 24139, 30891, 7454])"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "contexts_data[:5], targets[:5]  # list of lists"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f98f990b",
   "metadata": {},
   "source": [
    "## Dataset wrapper and dataloader creation\n",
    "#### dataloader essentially allows model to select data in batches and allow parallel proccessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "d9d6ab7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class Word2VecDataset(Dataset):\n",
    "    def __init__(self, contexts_data, targets):\n",
    "        self.contexts_data = torch.LongTensor(contexts_data)        # as they were initially list of lists\n",
    "        self.targets = torch.LongTensor(targets)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.targets)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.contexts_data[idx], self.targets[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "f7748c38",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = Word2VecDataset(contexts_data, targets)\n",
    "\n",
    "batch_size = 64\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "110da565",
   "metadata": {},
   "source": [
    "## Model Initialization and training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "12423652",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = W2V(vocab_size, emb_size).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "63053628",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_fn(model, optimizer, loss_fn, dataloader, device):\n",
    "\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "\n",
    "    for contexts, targets in dataloader:\n",
    "\n",
    "        contexts = contexts.to(device)\n",
    "        targets = targets.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs = model(contexts)\n",
    "        loss = loss_fn(outputs, targets)\n",
    "\n",
    "        loss.backward()\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        \n",
    "    return total_loss / len(dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "fd59642d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 01/70 | Average Training Loss: 4.8486\n",
      "Epoch 02/70 | Average Training Loss: 3.8633\n",
      "Epoch 03/70 | Average Training Loss: 3.2094\n",
      "Epoch 04/70 | Average Training Loss: 2.7709\n",
      "Epoch 05/70 | Average Training Loss: 2.4714\n",
      "Epoch 06/70 | Average Training Loss: 2.2579\n",
      "Epoch 07/70 | Average Training Loss: 2.1032\n",
      "Epoch 08/70 | Average Training Loss: 1.9801\n",
      "Epoch 09/70 | Average Training Loss: 1.8851\n",
      "Epoch 10/70 | Average Training Loss: 1.8084\n",
      "Epoch 11/70 | Average Training Loss: 1.7459\n",
      "Epoch 12/70 | Average Training Loss: 1.6946\n",
      "Epoch 13/70 | Average Training Loss: 1.6509\n",
      "Epoch 14/70 | Average Training Loss: 1.6123\n",
      "Epoch 15/70 | Average Training Loss: 1.5789\n",
      "Epoch 16/70 | Average Training Loss: 1.5513\n",
      "Epoch 17/70 | Average Training Loss: 1.5256\n",
      "Epoch 18/70 | Average Training Loss: 1.5026\n",
      "Epoch 19/70 | Average Training Loss: 1.4819\n",
      "Epoch 20/70 | Average Training Loss: 1.4648\n",
      "Epoch 21/70 | Average Training Loss: 1.4480\n",
      "Epoch 22/70 | Average Training Loss: 1.4305\n",
      "Epoch 23/70 | Average Training Loss: 1.4203\n",
      "Epoch 24/70 | Average Training Loss: 1.4056\n",
      "Epoch 25/70 | Average Training Loss: 1.3937\n",
      "Epoch 26/70 | Average Training Loss: 1.3836\n",
      "Epoch 27/70 | Average Training Loss: 1.3725\n",
      "Epoch 28/70 | Average Training Loss: 1.3630\n",
      "Epoch 29/70 | Average Training Loss: 1.3545\n",
      "Epoch 30/70 | Average Training Loss: 1.3453\n",
      "Epoch 31/70 | Average Training Loss: 1.3375\n",
      "Epoch 32/70 | Average Training Loss: 1.3309\n",
      "Epoch 33/70 | Average Training Loss: 1.3240\n",
      "Epoch 34/70 | Average Training Loss: 1.3168\n",
      "Epoch 35/70 | Average Training Loss: 1.3106\n",
      "Epoch 36/70 | Average Training Loss: 1.3036\n",
      "Epoch 37/70 | Average Training Loss: 1.2984\n",
      "Epoch 38/70 | Average Training Loss: 1.2937\n",
      "Epoch 39/70 | Average Training Loss: 1.2878\n",
      "Epoch 40/70 | Average Training Loss: 1.2834\n",
      "Epoch 41/70 | Average Training Loss: 1.2782\n",
      "Epoch 42/70 | Average Training Loss: 1.2736\n",
      "Epoch 43/70 | Average Training Loss: 1.2700\n",
      "Epoch 44/70 | Average Training Loss: 1.2657\n",
      "Epoch 45/70 | Average Training Loss: 1.2612\n",
      "Epoch 46/70 | Average Training Loss: 1.2582\n",
      "Epoch 47/70 | Average Training Loss: 1.2532\n",
      "Epoch 48/70 | Average Training Loss: 1.2505\n",
      "Epoch 49/70 | Average Training Loss: 1.2464\n",
      "Epoch 50/70 | Average Training Loss: 1.2436\n",
      "Epoch 51/70 | Average Training Loss: 1.2408\n",
      "Epoch 52/70 | Average Training Loss: 1.2374\n",
      "Epoch 53/70 | Average Training Loss: 1.2347\n",
      "Epoch 54/70 | Average Training Loss: 1.2320\n",
      "Epoch 55/70 | Average Training Loss: 1.2288\n",
      "Epoch 56/70 | Average Training Loss: 1.2251\n",
      "Epoch 57/70 | Average Training Loss: 1.2236\n",
      "Epoch 58/70 | Average Training Loss: 1.2204\n",
      "Epoch 59/70 | Average Training Loss: 1.2184\n",
      "Epoch 60/70 | Average Training Loss: 1.2141\n",
      "Epoch 61/70 | Average Training Loss: 1.2130\n",
      "Epoch 62/70 | Average Training Loss: 1.2100\n",
      "Epoch 63/70 | Average Training Loss: 1.2089\n",
      "Epoch 64/70 | Average Training Loss: 1.2068\n",
      "Epoch 65/70 | Average Training Loss: 1.2047\n",
      "Epoch 66/70 | Average Training Loss: 1.2029\n",
      "Epoch 67/70 | Average Training Loss: 1.2007\n",
      "Epoch 68/70 | Average Training Loss: 1.1980\n",
      "Epoch 69/70 | Average Training Loss: 1.1964\n",
      "Epoch 70/70 | Average Training Loss: 1.1946\n"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "num_epochs = 70\n",
    "optimizer = optim.Adam(params = model.parameters(), lr = 0.003)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "for epoch in range(num_epochs):\n",
    "    \n",
    "    avg_loss = train_fn(model, optimizer, loss_fn, dataloader, device)\n",
    "    \n",
    "    print(f\"Epoch {epoch+1:02d}/{num_epochs} | Average Training Loss: {avg_loss:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "1d26bcbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"word2vec_params.pth\")             # to save the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "3c80f7b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "W2V(\n",
       "  (embed): Embedding(34042, 200)\n",
       "  (out): Linear(in_features=200, out_features=34042, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loaded_model = W2V(vocab_size, emb_size) \n",
    "loaded_model.load_state_dict(torch.load(\"word2vec_params.pth\"))\n",
    "loaded_model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb27f169",
   "metadata": {},
   "source": [
    "## Example:\n",
    "#### checking cosine similarity between two similar words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "8b889c9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.0781], grad_fn=<SumBackward1>)\n"
     ]
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "embedding_weights = loaded_model.embed.weight\n",
    "idx1 = val2idx['iphone']\n",
    "idx2 = val2idx['apple']\n",
    "emb1 = embedding_weights[idx1]\n",
    "emb2 = embedding_weights[idx2]\n",
    "emb1_unsqueezed = emb1.unsqueeze(0)\n",
    "emb2_unsqueezed = emb2.unsqueeze(0)\n",
    "similarity = F.cosine_similarity(emb1_unsqueezed, emb2_unsqueezed)\n",
    "print(similarity)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e67e3c71",
   "metadata": {},
   "source": [
    "#### Better results could have been acheived with better tuning and longer training"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.13.3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
