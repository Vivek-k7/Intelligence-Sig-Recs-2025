{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9c25e23",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import random\n",
    "import math\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from torch.amp import autocast, GradScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "492ed5f9",
   "metadata": {},
   "source": [
    "# Data Preprocessing\n",
    "#### load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "282d6eee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['english', 'non_english'],\n",
      "        num_rows: 1966848\n",
      "    })\n",
      "})\n",
      "{'english': 'Resumption of the session', 'non_english': 'Reanudación del período de sesiones'}\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "ds = load_dataset(\"sentence-transformers/parallel-sentences-europarl\", \"en-es\")\n",
    "\n",
    "print(ds)\n",
    "print(ds['train'][0])\n",
    "dataset = ds['train']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dad4d57",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>english</th>\n",
       "      <th>non_english</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>According to the United Nations Food and Agric...</td>\n",
       "      <td>Según la Organización de las Naciones Unidas p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Once again, we see the conflict between indust...</td>\n",
       "      <td>Una vez más vemos surgir el conflicto entre la...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>All that has allowed us to understand one anot...</td>\n",
       "      <td>Todo esto ha hecho posible que nos conozcamos ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>The EU uses those expectations to demand refor...</td>\n",
       "      <td>La Unión Europea utiliza estas expectativas pa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The relevant paragraphs have been amended and ...</td>\n",
       "      <td>Se han modificado los párrafos pertinentes y y...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             english  \\\n",
       "0  According to the United Nations Food and Agric...   \n",
       "1  Once again, we see the conflict between indust...   \n",
       "2  All that has allowed us to understand one anot...   \n",
       "3  The EU uses those expectations to demand refor...   \n",
       "4  The relevant paragraphs have been amended and ...   \n",
       "\n",
       "                                         non_english  \n",
       "0  Según la Organización de las Naciones Unidas p...  \n",
       "1  Una vez más vemos surgir el conflicto entre la...  \n",
       "2  Todo esto ha hecho posible que nos conozcamos ...  \n",
       "3  La Unión Europea utiliza estas expectativas pa...  \n",
       "4  Se han modificado los párrafos pertinentes y y...  "
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subset = dataset.shuffle(seed=42).select(range(50000))\n",
    "subset = subset.to_pandas()\n",
    "subset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18342b5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "esp = subset['non_english'].to_list()\n",
    "esp_sizes = [len(i.split()) for i in esp]\n",
    "eng = subset['english'].to_list()\n",
    "eng_sizes = [len(i.split()) for i in eng]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81fc36a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1200x800 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "length_df = pd.DataFrame({'english':eng_sizes, 'spanish':esp_sizes})\n",
    "length_df.hist(bins = 40)\n",
    "plt.figure(figsize = (12,8))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5529f7e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "english    54.0\n",
      "spanish    57.0\n",
      "Name: 0.95, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "percentile_95 = length_df.quantile(0.95)\n",
    "print(percentile_95)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54e57c67",
   "metadata": {},
   "outputs": [],
   "source": [
    "import unicodedata\n",
    "import re\n",
    "\n",
    "def unicode_to_ascii(s):\n",
    "    normalized = unicodedata.normalize('NFD', s)\n",
    "    return ''.join(c for c in normalized if unicodedata.category(c) != 'Mn')              # done to convert special chars in spanish to ascii\n",
    "\n",
    "def process_text(text):\n",
    "    text = unicode_to_ascii(text.lower().strip())\n",
    "    text = re.sub(r'[^a-z?.!,¿]+', ' ', text)      # as character- ¿ belongs to spanish vocab\n",
    "    text = re.sub(r\"([?.!,¿])\", r\" \\1 \", text)     # adds extra space before the punctuation to tokenize lateron \n",
    "    text = re.sub(r'[\" \"]+', \" \", text)            # replaces multiple continuous white spaces with a single white space\n",
    "    text = '<sos> ' + text + ' <eos>'\n",
    "    return text                # start of sentence, end of sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5889f04a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('<sos> segun la organizacion de las naciones unidas para la agricultura y la alimentacion , la produccion de alimentos debe aumentar como minimo un para atender la creciente demanda de una poblacion mundial que se espera supere los millones de personas en .  <eos>',\n",
       " '<sos> according to the united nations food and agricultural organisation , food production must increase by at least to meet the growing demands of a world population which is expected to exceed billion by the year .  <eos>')"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "esp = [process_text(text) for text in esp]\n",
    "eng = [process_text(text) for text in eng]\n",
    "esp[0], eng[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6619eaac",
   "metadata": {},
   "source": [
    "## Vocab creation\n",
    "#### - same as previous notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd985aa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "def get_vocab(text_list):\n",
    "    all_words = [word for sentence in text_list for word in sentence.split()]\n",
    "\n",
    "    word_counts = Counter(all_words)\n",
    "\n",
    "    special_tokens = ['<pad>', '<unk>', '<sos>', '<eos>']\n",
    "    vocab = {token: i for i, token in enumerate(special_tokens)}\n",
    "\n",
    "    count = len(vocab)\n",
    "\n",
    "    for word, _ in word_counts.most_common():          # .most_common() to get words in order of their frequency\n",
    "        if word not in vocab:\n",
    "            vocab[word] = count\n",
    "            count+=1\n",
    "\n",
    "    idx2word = {i: word for word, i in vocab.items()}\n",
    "\n",
    "    return vocab, idx2word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d2868ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spanish vocabulary size: 28916\n",
      "English vocabulary size: 19013\n"
     ]
    }
   ],
   "source": [
    "word2idx_esp, idx2word_esp = get_vocab(esp)\n",
    "word2idx_eng, idx2word_eng = get_vocab(eng)\n",
    "print(f\"Spanish vocabulary size: {len(word2idx_esp)}\")\n",
    "print(f\"English vocabulary size: {len(word2idx_eng)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40b92088",
   "metadata": {},
   "outputs": [],
   "source": [
    "unk_idx_eng = word2idx_eng['<unk>']\n",
    "unk_idx_esp = word2idx_esp['<unk>']\n",
    "\n",
    "esp_transformed = []\n",
    "for sentence in esp:\n",
    "    indices = []\n",
    "    for word in sentence.split():\n",
    "        indices.append(word2idx_esp.get(word, unk_idx_esp))\n",
    "    esp_transformed.append(indices)\n",
    "\n",
    "eng_transformed = []\n",
    "for sentence in eng:\n",
    "    indices = []\n",
    "    for word in sentence.split():\n",
    "        indices.append(word2idx_eng.get(word, unk_idx_eng))\n",
    "    eng_transformed.append(indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afb9c95b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spanish Tensor Shape: torch.Size([57, 30000])\n",
      "tensor([[   2,    2,    2,  ...,    2,    2,    2],\n",
      "        [ 462,   19,   59,  ...,    9,  806,    6],\n",
      "        [   6,  104,   62,  ..., 1199,    8,  880],\n",
      "        ...,\n",
      "        [   0,    0,  509,  ...,    0,    0,   37],\n",
      "        [   0,    0,    5,  ...,    0,    0,    7],\n",
      "        [   0,    0,    6,  ...,    0,    0,    3]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "def pad_and_clip_sequences(sequences, max_len, padding_value):\n",
    "    padded_sequences = []\n",
    "    for seq in sequences:\n",
    "        if len(seq) > max_len:\n",
    "            clipped_seq = seq[:max_len]             #Clip the sequence\n",
    "            padded_sequences.append(clipped_seq)\n",
    "        else:\n",
    "            padded_seq = seq + [padding_value] * (max_len - len(seq))       # Pad the sequence\n",
    "            padded_sequences.append(padded_seq)\n",
    "            \n",
    "    return torch.tensor(padded_sequences, dtype=torch.long)\n",
    "\n",
    "MAX_LENGTH = 57\n",
    "PAD_IDX_ESP = word2idx_esp['<pad>'] \n",
    "PAD_IDX_ENG = word2idx_eng['<pad>'] \n",
    "\n",
    "# Apply padding and clipping\n",
    "esp_tensor = pad_and_clip_sequences(esp_transformed, MAX_LENGTH, PAD_IDX_ESP).transpose(0, 1)\n",
    "eng_tensor = pad_and_clip_sequences(eng_transformed, MAX_LENGTH, PAD_IDX_ENG).transpose(0, 1)\n",
    "\n",
    "print(\"Spanish Tensor Shape:\", esp_tensor.shape)\n",
    "print(esp_tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b06bf7d",
   "metadata": {},
   "source": [
    "# Model Architecture\n",
    "#### implementation of \"Attention is all you need\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68843b94",
   "metadata": {},
   "source": [
    "### A) Positional Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f318904",
   "metadata": {},
   "outputs": [],
   "source": [
    "def positional_encodings(seq_len, d_model):\n",
    "    pos = np.arange(seq_len)[:, np.newaxis]  #[[0], [1], [2], ..., [seq_len - 1]]\n",
    "    dims = np.arange(d_model)[np.newaxis, :] # [[0, 1, 2, ..., d_model - 1]]\n",
    "    \n",
    "    angles_input = pos/(np.power(10000, (2*(dims//2))/d_model))\n",
    "    pos_encodings = np.zeros((seq_len, d_model))\n",
    "    pos_encodings[:, 0::2] = np.sin(angles_input[:, 0::2])\n",
    "    pos_encodings[:, 1::2] = np.cos(angles_input[:, 1::2])\n",
    "\n",
    "    return torch.tensor(pos_encodings, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32928ff1",
   "metadata": {},
   "source": [
    "### B) MultiHeadAttention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca831d31",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_model, h):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.h = h\n",
    "        self.d_k = d_model // h \n",
    "\n",
    "        #Linear projections for Q, K, V\n",
    "        self.Wq = nn.Linear(d_model, d_model)\n",
    "        self.Wk = nn.Linear(d_model, d_model)\n",
    "        self.Wv = nn.Linear(d_model, d_model)\n",
    "        \n",
    "        #Final linear projection\n",
    "        self.W_o = nn.Linear(d_model, d_model)\n",
    "        \n",
    "    def forward(self, q, k, v, mask=None):\n",
    "        #Project Q, K, V\n",
    "        Q = self.Wq(q)                                     \n",
    "        K = self.Wk(k)                                     \n",
    "        V = self.Wv(v)                                     \n",
    "        \n",
    "        # Split into h heads\n",
    "        Q = self.split(Q)                               \n",
    "        K = self.split(K)  \n",
    "        V = self.split(V)\n",
    "\n",
    "        scores = torch.matmul(Q, K.transpose(-2,-1)) / math.sqrt(self.d_k)\n",
    "\n",
    "        if mask is not None:\n",
    "            scores = scores.masked_fill(mask == 0, -10000)\n",
    "        \n",
    "        out = torch.matmul(scores.softmax(dim = -1), V)\n",
    "        # Concatenate heads\n",
    "        batch_size, _, length, _ = out.size()\n",
    "        out = out.transpose(1,2).contiguous().view(batch_size, length, self.d_model) \n",
    "        \n",
    "        # Final projection\n",
    "        out = self.W_o(out)  \n",
    "        return out\n",
    "    \n",
    "    def split(self, tensor):\n",
    "        batch_size, length, _ = tensor.size()\n",
    "        tensor = tensor.view(batch_size, length, self.h, self.d_k).transpose(1,2)\n",
    "\n",
    "        return tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5231a41",
   "metadata": {},
   "source": [
    "### C) Feed forward Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b41e3dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedFwd(nn.Module):\n",
    "    def __init__(self, d_model, prob= 0.1):\n",
    "        super().__init__()\n",
    "        self.linear1 = nn.Linear(d_model, d_model*2)\n",
    "        self.dropout = nn.Dropout(p=prob)\n",
    "        self.linear2 = nn.Linear(2*d_model, d_model)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.dropout(nn.functional.relu(self.linear1(x)))\n",
    "        x = self.linear2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d990c8ed",
   "metadata": {},
   "source": [
    "## Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "317c029b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderBlock(nn.Module):\n",
    "    def __init__(self, d_model, h, prob= 0.1):\n",
    "        super().__init__()\n",
    "        self.mha = MultiHeadAttention(d_model, h)\n",
    "        self.dropout = nn.Dropout(p= prob)\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.ffd = FeedFwd(d_model)       \n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "\n",
    "    def forward(self, x, src_mask):\n",
    "        x = self.norm1(self.dropout(x + self.mha(x, x, x, src_mask)))   # src mask for preventing attention towards padding tokens\n",
    "        x = self.norm2(x + self.ffd(x))  \n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dcefb05",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model, h, device, dropout=0.2):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.device = device\n",
    "        self.embeddings = nn.Embedding(vocab_size, d_model)\n",
    "\n",
    "        self.block1 = EncoderBlock(d_model, h)        # 2 encoder blocks\n",
    "        self.block2 = EncoderBlock(d_model, h)\n",
    "\n",
    "    def forward(self, x, src_mask):\n",
    "        x = self.embeddings(x) * (self.d_model ** 0.5)\n",
    "        pos_enc = positional_encodings(x.size(1), self.d_model).to(x.device) \n",
    "        x = x + pos_enc.unsqueeze(0)  \n",
    "\n",
    "        # pass through encoder blocks\n",
    "        x = self.block1(x, src_mask)\n",
    "        x = self.block2(x, src_mask)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0296595b",
   "metadata": {},
   "source": [
    "## Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a1f43eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderBlock(nn.Module):\n",
    "    def __init__(self, d_model, h, device, prob=0.1):\n",
    "        super().__init__()\n",
    "        self.self_att = MultiHeadAttention(d_model, h)\n",
    "        self.dropout1 = nn.Dropout(p=prob)\n",
    "        self.layernorm1 = nn.LayerNorm(d_model)\n",
    "\n",
    "        self.cross_att = MultiHeadAttention(d_model, h)\n",
    "        self.dropout2 = nn.Dropout(p=0)\n",
    "        self.layernorm2 = nn.LayerNorm(d_model)\n",
    "\n",
    "        self.ffd = FeedFwd(d_model)\n",
    "        self.dropout3 = nn.Dropout(p=0)\n",
    "        self.layernorm3 = nn.LayerNorm(d_model)\n",
    "\n",
    "    def forward(self, x, enc, trg_mask, src_mask):\n",
    "        x_initial = x\n",
    "        x = self.self_att(q=x, k=x, v=x, mask=trg_mask)\n",
    "        x = self.layernorm1(x_initial + self.dropout1(x))\n",
    "\n",
    "        if enc is not None:\n",
    "            x_initial = x\n",
    "            x = self.cross_att(q=x, k=enc, v=enc, mask=src_mask)\n",
    "            x = self.layernorm2(x_initial + self.dropout2(x))\n",
    "\n",
    "        x_initial = x\n",
    "        x = self.ffd(x)\n",
    "        x = self.layernorm3(x_initial + self.dropout3(x))\n",
    "        \n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "672cb48e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model, h, device, prob = 0.1):\n",
    "        super().__init__()\n",
    "        self.embeddings = nn.Embedding(vocab_size, d_model)\n",
    "        self.block1 = DecoderBlock(d_model, h, device, prob=0.1)\n",
    "        self.block2 = DecoderBlock(d_model, h, device, prob=0.0)\n",
    "        self.linear = nn.Linear(d_model, vocab_size)\n",
    "        self.d_model = d_model\n",
    "\n",
    "\n",
    "    def forward(self, x, enc_src, trg_mask, src_mask):\n",
    "\n",
    "        x = self.embeddings(x) * (self.d_model ** 0.5)\n",
    "        pos_enc = positional_encodings(x.size(1), self.d_model).to(x.device) \n",
    "        x = x + pos_enc.unsqueeze(0)\n",
    "        \n",
    "        x = self.block1(x, enc_src, trg_mask, src_mask)\n",
    "        x = self.block2(x, enc_src, trg_mask, src_mask)\n",
    "\n",
    "        x = self.linear(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2238c2c1",
   "metadata": {},
   "source": [
    "## Final Translator model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c01b83c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class translator(nn.Module):\n",
    "    def __init__(self, src_pad_idx, trg_pad_idx, src_vocab_size, trg_vocab_size, d_model, h, device):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.encoder = Encoder(src_vocab_size, d_model, h, device)\n",
    "        self.decoder = Decoder(trg_vocab_size, d_model, h, device)\n",
    "        self.device = device    \n",
    "        self.src_pad_idx = src_pad_idx\n",
    "        self.trg_pad_idx = trg_pad_idx\n",
    "        self.device = device\n",
    "        \n",
    "    def create_src_mask(self, src):\n",
    "        src_mask = (src != self.src_pad_idx).unsqueeze(1).unsqueeze(2)\n",
    "        return src_mask\n",
    "    \n",
    "    def create_trg_mask(self, trg):                                                     # to prevent attention to padding and attending to future tokens \n",
    "        trg_pad_mask = (trg != self.trg_pad_idx).unsqueeze(1).unsqueeze(3)\n",
    "        trg_len = trg.shape[1]\n",
    "        trg_sub_mask = torch.tril(torch.ones(trg_len, trg_len, dtype=torch.bool)).to(self.device)\n",
    "        trg_mask = trg_pad_mask & trg_sub_mask\n",
    "        return trg_mask\n",
    "\n",
    "    def forward(self, src, trg):\n",
    "        src_mask = self.create_src_mask(src)\n",
    "        trg_mask = self.create_trg_mask(trg)\n",
    "        enc_src = self.encoder(src, src_mask)\n",
    "        outputs = self.decoder(trg, enc_src, trg_mask, src_mask)\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a01a6348",
   "metadata": {},
   "source": [
    "# Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "789d8f62",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "d_model = 256\n",
    "h = 4\n",
    "drop_prob = 0.1\n",
    "\n",
    "# optimizer parameter setting\n",
    "init_lr = 5e-5\n",
    "factor = 0.6\n",
    "epochs = 20\n",
    "clip = 1.0\n",
    "weight_decay = 5e-6\n",
    "inf = float('inf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e15ed365",
   "metadata": {},
   "outputs": [],
   "source": [
    "src_pad_idx = word2idx_esp['<pad>']\n",
    "trg_pad_idx = word2idx_eng['<pad>']\n",
    "\n",
    "src_vocab_size = len(word2idx_esp)\n",
    "trg_vocab_size = len(word2idx_eng)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca5b9c07",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.amp import autocast, GradScaler\n",
    "from tqdm import tqdm\n",
    "import math\n",
    "import time\n",
    "\n",
    "def train_epoch(model, train_loader, valid_loader, optimizer, criterion, clip, scaler, scheduler, device):\n",
    "    # Training \n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    train_steps = 0 \n",
    "    \n",
    "    for src, trg in tqdm(train_loader, desc=\"Training\", leave=False):\n",
    "        src = src.to(device)\n",
    "        trg = trg.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        with autocast('cuda'):\n",
    "            outputs = model(src, trg[:, :-1])  \n",
    "            output_dim = outputs.shape[-1]\n",
    "            output_reshape = outputs.contiguous().view(-1, output_dim)\n",
    "            trg_reshaped = trg[:, 1:].contiguous().view(-1)\n",
    "            loss = criterion(output_reshape, trg_reshaped)\n",
    "        \n",
    "        scaler.scale(loss).backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        train_loss += loss.item()\n",
    "    \n",
    "    avg_train_loss = train_loss / len(train_loader)\n",
    "    \n",
    "    # Validation \n",
    "    model.eval()\n",
    "    valid_loss = 0.0\n",
    "    logits_std_sum = 0.0\n",
    "    num_batches = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for src, trg in tqdm(valid_loader, desc=\"Validation\", leave=False):\n",
    "            src = src.to(device)\n",
    "            trg = trg.to(device)\n",
    "            \n",
    "            with autocast('cuda'):\n",
    "                outputs = model(src, trg[:, :-1])\n",
    "                output_dim = outputs.shape[-1]\n",
    "                output_reshaped = outputs.contiguous().view(-1, output_dim)\n",
    "                trg_reshaped = trg[:, 1:].contiguous().view(-1)\n",
    "                loss = criterion(output_reshaped, trg_reshaped)\n",
    "            \n",
    "            valid_loss += loss.item()\n",
    "            logits_std_sum += outputs.std().item()  # Monitor if logits diversify (>0.1+ indicates learning)\n",
    "            num_batches += 1\n",
    "    \n",
    "    avg_valid_loss = valid_loss / len(valid_loader)\n",
    "    avg_logits_std = logits_std_sum / num_batches\n",
    "    \n",
    "    # Print diagnostics\n",
    "    current_lr = optimizer.param_groups[0]['lr']\n",
    "    print(f\"Current LR: {current_lr:.2e} | Avg Logits Std: {avg_logits_std:.4f}\")\n",
    "    \n",
    "    return avg_train_loss, avg_valid_loss\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "487c480b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import time\n",
    "\n",
    "from torch import nn, optim\n",
    "from torch.optim import Adam\n",
    "model = translator(src_pad_idx, trg_pad_idx, src_vocab_size, trg_vocab_size, d_model, h, device).to(device)\n",
    "\n",
    "def initialize_weights(m):\n",
    "    if hasattr(m, 'weight') and m.weight.dim() > 1:\n",
    "        nn.init.xavier_uniform_(m.weight)\n",
    "\n",
    "model.apply(initialize_weights)\n",
    "\n",
    "optimizer = Adam(params=model.parameters(), lr=init_lr, weight_decay=weight_decay)\n",
    "\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer=optimizer, factor=factor)\n",
    "\n",
    "scaler = GradScaler('cuda')\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=trg_pad_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "114fad7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "src_tensor = torch.LongTensor(esp_tensor)\n",
    "trg_tensor = torch.LongTensor(eng_tensor)\n",
    "\n",
    "train_src_tensor, valid_src_tensor, train_trg_tensor, valid_trg_tensor = train_test_split(\n",
    "    src_tensor.transpose(0, 1),  \n",
    "    trg_tensor.transpose(0, 1),  \n",
    "    test_size=0.1,\n",
    "    random_state=42\n",
    ")\n",
    "train_dataset = TensorDataset(train_src_tensor, train_trg_tensor)\n",
    "valid_dataset = TensorDataset(valid_src_tensor, valid_trg_tensor)\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "    train_dataset, \n",
    "    batch_size=batch_size, \n",
    "    shuffle=True, \n",
    "    num_workers=4,\n",
    "    pin_memory=True\n",
    ")\n",
    "\n",
    "valid_dataloader = DataLoader(\n",
    "    valid_dataset, \n",
    "    batch_size=batch_size, \n",
    "    num_workers=4, \n",
    "    pin_memory=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83d49244",
   "metadata": {},
   "outputs": [],
   "source": [
    "def epoch_time(start_time, end_time):\n",
    "    # This calculates the total duration in seconds. \n",
    "    elapsed_time = end_time - start_time\n",
    "    \n",
    "    # This converts the total seconds to whole minutes by dividing by 60 \n",
    "    elapsed_mins = int(elapsed_time / 60)\n",
    "    \n",
    "    # This calculates the remaining seconds. \n",
    "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
    "    \n",
    "    # It returns the calculated minutes and seconds.\n",
    "    return elapsed_mins, elapsed_secs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd2ed887",
   "metadata": {},
   "source": [
    "# Training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c78fe97",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current LR: 5.00e-05 | Avg Logits Std: 0.1181\n",
      "Epoch: 01 | Time: 0m 24s\n",
      "\tTrain Loss: 8.998 | Train PPL: 8083.429\n",
      "\tValid Loss: 8.172 | Valid PPL: 3538.895\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current LR: 5.00e-05 | Avg Logits Std: 0.2020\n",
      "Epoch: 02 | Time: 0m 24s\n",
      "\tTrain Loss: 7.662 | Train PPL: 2126.990\n",
      "\tValid Loss: 7.280 | Valid PPL: 1451.206\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current LR: 5.00e-05 | Avg Logits Std: 0.2129\n",
      "Epoch: 03 | Time: 0m 27s\n",
      "\tTrain Loss: 7.192 | Train PPL: 1329.071\n",
      "\tValid Loss: 7.155 | Valid PPL: 1280.858\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current LR: 5.00e-05 | Avg Logits Std: 0.2057\n",
      "Epoch: 04 | Time: 0m 29s\n",
      "\tTrain Loss: 7.146 | Train PPL: 1268.500\n",
      "\tValid Loss: 7.145 | Valid PPL: 1268.233\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current LR: 5.00e-05 | Avg Logits Std: 0.1552\n",
      "Epoch: 05 | Time: 0m 29s\n",
      "\tTrain Loss: 7.191 | Train PPL: 1326.779\n",
      "\tValid Loss: 7.520 | Valid PPL: 1843.896\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current LR: 5.00e-05 | Avg Logits Std: 0.1058\n",
      "Epoch: 06 | Time: 0m 29s\n",
      "\tTrain Loss: 7.888 | Train PPL: 2663.962\n",
      "\tValid Loss: 8.098 | Valid PPL: 3289.069\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current LR: 5.00e-05 | Avg Logits Std: 0.1001\n",
      "Epoch: 07 | Time: 0m 28s\n",
      "\tTrain Loss: 8.148 | Train PPL: 3456.129\n",
      "\tValid Loss: 8.184 | Valid PPL: 3581.414\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current LR: 5.00e-05 | Avg Logits Std: 0.0994\n",
      "Epoch: 08 | Time: 0m 28s\n",
      "\tTrain Loss: 8.181 | Train PPL: 3571.583\n",
      "\tValid Loss: 8.197 | Valid PPL: 3628.533\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current LR: 5.00e-05 | Avg Logits Std: 0.0980\n",
      "Epoch: 09 | Time: 0m 29s\n",
      "\tTrain Loss: 8.196 | Train PPL: 3626.934\n",
      "\tValid Loss: 8.218 | Valid PPL: 3705.658\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current LR: 5.00e-05 | Avg Logits Std: 0.0629\n",
      "Epoch: 10 | Time: 0m 29s\n",
      "\tTrain Loss: 8.382 | Train PPL: 4366.271\n",
      "\tValid Loss: 8.770 | Valid PPL: 6436.572\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current LR: 5.00e-05 | Avg Logits Std: 0.0485\n",
      "Epoch: 11 | Time: 0m 29s\n",
      "\tTrain Loss: 8.915 | Train PPL: 7443.348\n",
      "\tValid Loss: 9.007 | Valid PPL: 8160.596\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current LR: 5.00e-05 | Avg Logits Std: 0.0454\n",
      "Epoch: 12 | Time: 0m 28s\n",
      "\tTrain Loss: 9.030 | Train PPL: 8353.463\n",
      "\tValid Loss: 9.058 | Valid PPL: 8590.664\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current LR: 5.00e-05 | Avg Logits Std: 0.0440\n",
      "Epoch: 13 | Time: 0m 28s\n",
      "\tTrain Loss: 9.065 | Train PPL: 8643.803\n",
      "\tValid Loss: 9.083 | Valid PPL: 8800.414\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current LR: 5.00e-05 | Avg Logits Std: 0.0429\n",
      "Epoch: 14 | Time: 0m 28s\n",
      "\tTrain Loss: 9.087 | Train PPL: 8837.371\n",
      "\tValid Loss: 9.103 | Valid PPL: 8983.824\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current LR: 3.00e-05 | Avg Logits Std: 0.0226\n",
      "Epoch: 15 | Time: 0m 28s\n",
      "\tTrain Loss: 9.288 | Train PPL: 10812.627\n",
      "\tValid Loss: 9.457 | Valid PPL: 12792.342\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current LR: 3.00e-05 | Avg Logits Std: 0.0207\n",
      "Epoch: 16 | Time: 0m 28s\n",
      "\tTrain Loss: 9.473 | Train PPL: 13009.980\n",
      "\tValid Loss: 9.490 | Valid PPL: 13225.156\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current LR: 3.00e-05 | Avg Logits Std: 0.0201\n",
      "Epoch: 17 | Time: 0m 28s\n",
      "\tTrain Loss: 9.493 | Train PPL: 13264.575\n",
      "\tValid Loss: 9.501 | Valid PPL: 13369.803\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current LR: 3.00e-05 | Avg Logits Std: 0.0197\n",
      "Epoch: 18 | Time: 0m 28s\n",
      "\tTrain Loss: 9.501 | Train PPL: 13379.172\n",
      "\tValid Loss: 9.508 | Valid PPL: 13465.711\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current LR: 3.00e-05 | Avg Logits Std: 0.0193\n",
      "Epoch: 19 | Time: 0m 28s\n",
      "\tTrain Loss: 9.508 | Train PPL: 13467.667\n",
      "\tValid Loss: 9.514 | Valid PPL: 13547.539\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current LR: 3.00e-05 | Avg Logits Std: 0.0190\n",
      "Epoch: 20 | Time: 0m 28s\n",
      "\tTrain Loss: 9.514 | Train PPL: 13551.415\n",
      "\tValid Loss: 9.520 | Valid PPL: 13635.411\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "best_valid_loss = float('inf')\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    start_time = time.time()\n",
    "    \n",
    "    train_loss, valid_loss = train_epoch(\n",
    "        model, train_dataloader, valid_dataloader, optimizer, criterion, clip, scaler, scheduler, device\n",
    "    )\n",
    "    \n",
    "    end_time = time.time()\n",
    "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "    \n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "        torch.save(model.state_dict(), 'best_model.pt')\n",
    "    \n",
    "    if (epoch + 1) % 5 == 0:\n",
    "        torch.save(model.state_dict(), f'model_epoch_{epoch+1:02d}.pt')\n",
    "    \n",
    "    print(f'Epoch: {epoch+1:02} | Time: {epoch_mins}m {epoch_secs}s')\n",
    "    print(f'\\tTrain Loss: {train_loss:.3f} | Train PPL: {math.exp(train_loss):7.3f}')\n",
    "    print(f'\\tValid Loss: {valid_loss:.3f} | Valid PPL: {math.exp(valid_loss):7.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c98a02f9",
   "metadata": {},
   "source": [
    "##### Could not tune the parameters due to few issues with my laptop"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.13.3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
