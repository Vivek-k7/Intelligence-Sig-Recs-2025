{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31b55c9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import time\n",
    "import random\n",
    "import math\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cbd9b24",
   "metadata": {},
   "source": [
    "# Data Preprocessing\n",
    "#### load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "eb44962f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Personal\\code\\ML comp\\New folder\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['english', 'non_english'],\n",
      "        num_rows: 1966848\n",
      "    })\n",
      "})\n",
      "{'english': 'Resumption of the session', 'non_english': 'Reanudación del período de sesiones'}\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "ds = load_dataset(\"sentence-transformers/parallel-sentences-europarl\", \"en-es\")\n",
    "\n",
    "print(ds)\n",
    "print(ds['train'][0])\n",
    "dataset = ds['train']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2b031df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>english</th>\n",
       "      <th>non_english</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>According to the United Nations Food and Agric...</td>\n",
       "      <td>Según la Organización de las Naciones Unidas p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Once again, we see the conflict between indust...</td>\n",
       "      <td>Una vez más vemos surgir el conflicto entre la...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>All that has allowed us to understand one anot...</td>\n",
       "      <td>Todo esto ha hecho posible que nos conozcamos ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>The EU uses those expectations to demand refor...</td>\n",
       "      <td>La Unión Europea utiliza estas expectativas pa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The relevant paragraphs have been amended and ...</td>\n",
       "      <td>Se han modificado los párrafos pertinentes y y...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             english  \\\n",
       "0  According to the United Nations Food and Agric...   \n",
       "1  Once again, we see the conflict between indust...   \n",
       "2  All that has allowed us to understand one anot...   \n",
       "3  The EU uses those expectations to demand refor...   \n",
       "4  The relevant paragraphs have been amended and ...   \n",
       "\n",
       "                                         non_english  \n",
       "0  Según la Organización de las Naciones Unidas p...  \n",
       "1  Una vez más vemos surgir el conflicto entre la...  \n",
       "2  Todo esto ha hecho posible que nos conozcamos ...  \n",
       "3  La Unión Europea utiliza estas expectativas pa...  \n",
       "4  Se han modificado los párrafos pertinentes y y...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subset = dataset.shuffle(seed=42).select(range(40000))           # take only a sample\n",
    "subset = subset.to_pandas()\n",
    "subset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0427f95d",
   "metadata": {},
   "outputs": [],
   "source": [
    "esp = subset['non_english'].to_list()\n",
    "esp_sizes = [len(i.split()) for i in esp]                 # get no no. of words for each sentences\n",
    "eng = subset['english'].to_list()\n",
    "eng_sizes = [len(i.split()) for i in eng]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1ffe2994",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1200x800 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "length_df = pd.DataFrame({'english':eng_sizes, 'spanish':esp_sizes})\n",
    "length_df.hist(bins = 40)\n",
    "plt.figure(figsize = (12,8))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b77e132e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "english    54.0\n",
      "spanish    56.0\n",
      "Name: 0.95, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "percentile_95 = length_df.quantile(0.95)                # considering 95th percentile to reduce computation\n",
    "print(percentile_95)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e9c5b9b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import unicodedata\n",
    "import re\n",
    "\n",
    "def unicode_to_ascii(s):\n",
    "    normalized = unicodedata.normalize('NFD', s)\n",
    "    return ''.join(c for c in normalized if unicodedata.category(c) != 'Mn')              # done to convert special chars in spanish to ascii\n",
    "\n",
    "def process_text(text):\n",
    "    text = unicode_to_ascii(text.lower().strip())\n",
    "    text = re.sub(r'[^a-z?.!,¿]+', ' ', text)      # as character- ¿ belongs to spanish vocab\n",
    "    text = re.sub(r\"([?.!,¿])\", r\" \\1 \", text)     # adds extra space before the punctuation to tokenize lateron \n",
    "    text = re.sub(r'[\" \"]+', \" \", text)            # replaces multiple continuous white spaces with a single white space\n",
    "    text = '<sos> ' + text + ' <eos>'\n",
    "    return text                # start of sentence, end of sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9b41c66",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('<sos> segun la organizacion de las naciones unidas para la agricultura y la alimentacion , la produccion de alimentos debe aumentar como minimo un para atender la creciente demanda de una poblacion mundial que se espera supere los millones de personas en .  <eos>',\n",
       " '<sos> according to the united nations food and agricultural organisation , food production must increase by at least to meet the growing demands of a world population which is expected to exceed billion by the year .  <eos>')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "esp = [process_text(text) for text in esp]\n",
    "eng = [process_text(text) for text in eng]\n",
    "esp[0], eng[0]     # example of processed data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fd019ad",
   "metadata": {},
   "source": [
    "## Vocab creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3e4acba",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "def get_vocab(text_list):\n",
    "    all_words = [word for sentence in text_list for word in sentence.split()]              # all words in entire corpus\n",
    "\n",
    "    word_counts = Counter(all_words)\n",
    "\n",
    "    special_tokens = ['<pad>', '<unk>', '<sos>', '<eos>']\n",
    "    vocab = {token: i for i, token in enumerate(special_tokens)}\n",
    "\n",
    "    count = len(vocab)\n",
    "\n",
    "    for word, _ in word_counts.most_common():          # .most_common() to get words in order of their frequency\n",
    "        if word not in vocab:\n",
    "            vocab[word] = count\n",
    "            count+=1\n",
    "\n",
    "    idx2word = {i: word for word, i in vocab.items()}\n",
    "\n",
    "    return vocab, idx2word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ed299be3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spanish vocabulary size: 32820\n",
      "English vocabulary size: 21258\n"
     ]
    }
   ],
   "source": [
    "word2idx_esp, idx2word_esp = get_vocab(esp)\n",
    "word2idx_eng, idx2word_eng = get_vocab(eng)\n",
    "print(f\"Spanish vocabulary size: {len(word2idx_esp)}\")\n",
    "print(f\"English vocabulary size: {len(word2idx_eng)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfef4b1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "unk_idx_eng = word2idx_eng['<unk>']\n",
    "unk_idx_esp = word2idx_esp['<unk>']\n",
    "\n",
    "esp_transformed = []\n",
    "for sentence in esp:\n",
    "    indices = []\n",
    "    for word in sentence.split():\n",
    "        indices.append(word2idx_esp.get(word, unk_idx_esp))  # idx of word if present in vocab; if not- idx of UNK token\n",
    "    esp_transformed.append(indices)\n",
    "\n",
    "eng_transformed = []\n",
    "for sentence in eng:\n",
    "    indices = []\n",
    "    for word in sentence.split():\n",
    "        indices.append(word2idx_eng.get(word, unk_idx_eng))\n",
    "    eng_transformed.append(indices)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "675fdbf9",
   "metadata": {},
   "source": [
    "#### A fixed sequence length is still required in LSTMs even if they are sequential, as the batching and GPU processing need fixed matrices. Obviously not all statements can be of same length so we pad the ones that are shorter and clip the ones that are longer than the 95th percentile sentence length found earlier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f83e720",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spanish Tensor Shape: torch.Size([57, 40000])\n",
      "tensor([[   2,    2,    2,  ...,    2,    2,    2],\n",
      "        [ 472,   19,   60,  ...,   18,   26,   18],\n",
      "        [   6,  109,   64,  ...,   58, 1679,  323],\n",
      "        ...,\n",
      "        [   0,    0,  503,  ...,    0,    0,    0],\n",
      "        [   0,    0,    5,  ...,    0,    0,    0],\n",
      "        [   0,    0,    6,  ...,    0,    0,    0]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "def pad_and_clip_sequences(sequences, max_len, padding_value):\n",
    "    padded_sequences = []\n",
    "    for seq in sequences:\n",
    "        if len(seq) > max_len:\n",
    "            clipped_seq = seq[:max_len]             #Clip the sequence\n",
    "            padded_sequences.append(clipped_seq)\n",
    "        else:\n",
    "            padded_seq = seq + [padding_value] * (max_len - len(seq))       # Pad the sequence\n",
    "            padded_sequences.append(padded_seq)\n",
    "            \n",
    "    return torch.tensor(padded_sequences, dtype=torch.long)\n",
    "\n",
    "MAX_LENGTH = 57 # from the 95th percentile\n",
    "PAD_IDX_ESP = word2idx_esp['<pad>'] \n",
    "PAD_IDX_ENG = word2idx_eng['<pad>'] \n",
    "\n",
    "# Apply padding and clipping\n",
    "esp_tensor = pad_and_clip_sequences(esp_transformed, MAX_LENGTH, PAD_IDX_ESP).transpose(0, 1)\n",
    "eng_tensor = pad_and_clip_sequences(eng_transformed, MAX_LENGTH, PAD_IDX_ENG).transpose(0, 1)\n",
    "\n",
    "print(\"Spanish Tensor Shape:\", esp_tensor.shape)\n",
    "print(esp_tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "582d397b",
   "metadata": {},
   "source": [
    "# Model Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6a4483e",
   "metadata": {},
   "source": [
    "## Encoder\n",
    "#### Takes in the indices of the tokens (referring to the vocabulary) from the sentence, and gets the embeddings from the nn.Embedding layer that is (vocab_size x embed_dimensions) then passes it through the LSTM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "134db58c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, esp_vocab_size, embed_dim, hidden_dim, n_layers, dropout):\n",
    "        super().__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.n_layers = n_layers\n",
    "        self.embedding = nn.Embedding(esp_vocab_size, embed_dim)\n",
    "        self.rnn = nn.LSTM(embed_dim, hidden_dim, n_layers, dropout=dropout)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, esp_indices):\n",
    "\n",
    "        embedded = self.dropout(self.embedding(esp_indices))        # embedded = [esp_indices length, batch size, embedding dim]\n",
    "        \n",
    "        outputs, (hidden, cell) = self.rnn(embedded)\n",
    "        # outputs = [esp_indices length, batch size, hidden dim]    - used for attention mechanism \n",
    "        # hidden = [n layers, batch size, hidden dim]               - final hidden state after processing the last input token  (working memory)\n",
    "        # cell = [n layers, batch size, hidden dim]                 - long term memory after processing the last input token    (long term memory)\n",
    "        return outputs, hidden, cell                                "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b9fa228",
   "metadata": {},
   "source": [
    "## Attention Block"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "015630ed",
   "metadata": {},
   "source": [
    "#### Takes current hidden Decoder state (x) and the outputs from encoder (which has information about each processed input token); measures how relevant each of the input token is to the decoder's current hidden state by concatenating both and passing through a linear layer with a tanh function that gives a tensor which is sort of a score for each token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5cbc0f51",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    def __init__(self, hidden_dim):\n",
    "        super().__init__()\n",
    "        self.attn = nn.Linear(hidden_dim * 2, hidden_dim)\n",
    "        self.out = nn.Linear(hidden_dim, 1, bias = False)\n",
    "\n",
    "    def forward(self, x, encoder_outputs, mask):\n",
    "        # x = [batch_size, hidden_dim]  - the current hidden state of the decoder\n",
    "        # encoder_outputs = [esp_indices length, batch_size, hidden_dim]\n",
    "        # mask = [batch_size, src_len]\n",
    "        \n",
    "        esp_indices_len = encoder_outputs.shape[0]\n",
    "        \n",
    "        x = x.unsqueeze(1).repeat(1, esp_indices_len, 1)\n",
    "        encoder_outputs = encoder_outputs.permute(1, 0, 2)\n",
    "        \n",
    "        x = torch.tanh(self.attn(torch.cat((x, encoder_outputs), dim = 2)))\n",
    "        \n",
    "        attention = self.out(x).squeeze(2)\n",
    "        \n",
    "        attention = attention.masked_fill(mask == 0, -10000)                # Apply the mask\n",
    "        \n",
    "        return F.softmax(attention, dim=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad5db76f",
   "metadata": {},
   "source": [
    "## Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeeb2513",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, eng_vocab_size, embed_dim, hidden_dim, n_layers, dropout, attention):\n",
    "        super().__init__()\n",
    "        self.attention = attention\n",
    "        self.output_dim = eng_vocab_size\n",
    "        self.embedding = nn.Embedding(self.output_dim, embed_dim)\n",
    "        self.rnn = nn.LSTM(hidden_dim + embed_dim, hidden_dim, n_layers, dropout=dropout)        # to concatenate the context vector() from attention, extra hidden_dim, with  \n",
    "        self.fc_out = nn.Linear(hidden_dim * 2 + embed_dim, self.output_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, hidden, cell, encoder_outputs, mask):\n",
    "        # x = [batch_size] - the first tokens in each target sentence of the batch; used as input to generate the output token for each time step of the decoder\n",
    "        \n",
    "        x = x.unsqueeze(0)          # x = [1, batch_size]\n",
    "        embedded = self.dropout(self.embedding(x))      \n",
    "        \n",
    "        a = self.attention(hidden[-1], encoder_outputs, mask)               #attension weights\n",
    "        a = a.unsqueeze(1) # a = [batch_size, 1, esp_indices_len]\n",
    "        \n",
    "        encoder_outputs = encoder_outputs.permute(1, 0, 2) # [batch_size, esp_indices_len, hidden_dim]\n",
    "        \n",
    "        weighted = a @ encoder_outputs # weighted = [batch_size, 1, hidden_dim]\n",
    "        weighted = weighted.permute(1, 0, 2) # weighted = [1, batch_size, hidden_dim]\n",
    "        \n",
    "        rnn_input = torch.cat((embedded, weighted), dim = 2)      # Concatenate embedding and weighted context for the LSTM\n",
    "        \n",
    "        output, (hidden, cell) = self.rnn(rnn_input, (hidden, cell))\n",
    "        \n",
    "        embedded = embedded.squeeze(0)\n",
    "        output = output.squeeze(0)\n",
    "        weighted = weighted.squeeze(0)\n",
    "        \n",
    "        prediction = self.fc_out(torch.cat((output, weighted, embedded), dim = 1))                # Concatenate everything for the final prediction\n",
    "        \n",
    "        return prediction, hidden, cell, a.squeeze(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31d93223",
   "metadata": {},
   "source": [
    "## Final Translator "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47d64015",
   "metadata": {},
   "source": [
    "#### We use teacher forcing in training: i.e, for input in time step t of the discriminator, we provide actual target of time step T= t-1 instead of decoder's output at T=t-1. But only with a probability instead of everytime, as it gives more flexibility for the model to learn to adapt to certain mismatches so that it doesnt cause much of a problem during testing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "388fe56c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class translator(nn.Module):\n",
    "    def __init__(self, encoder, decoder, pad_idx, device):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.encoder = encoder  \n",
    "        self.decoder = decoder\n",
    "        self.esp_pad_idx = pad_idx\n",
    "        self.device = device\n",
    "        \n",
    "    def create_mask(self, esp_indices):\n",
    "        mask = (esp_indices != self.esp_pad_idx).permute(1, 0)      # mask is done to ignore attending to pad tokens\n",
    "        return mask\n",
    "\n",
    "    def forward(self, esp_indices, eng_indices, teacher_forcing_ratio = 0.5):\n",
    "        \n",
    "        batch_size = eng_indices.shape[1]\n",
    "        trg_len = eng_indices.shape[0]\n",
    "        trg_vocab_size = self.decoder.output_dim\n",
    "        outputs = torch.zeros(trg_len, batch_size, trg_vocab_size).to(self.device)\n",
    "        \n",
    "        encoder_outputs, hidden, cell = self.encoder(esp_indices)\n",
    "        \n",
    "        mask = self.create_mask(esp_indices)         # the mask for the source sentence\n",
    "        \n",
    "        x = eng_indices[0,:]                         # take the first word from each sentence (batch)\n",
    "        \n",
    "        for t in range(1,trg_len):                     # loop required as decoder outputs one token at a time\n",
    "            \n",
    "            output, hidden, cell, _ = self.decoder(x, hidden, cell, encoder_outputs, mask)             \n",
    "            \n",
    "            outputs[t] = output\n",
    "            \n",
    "            teacher_force = random.random() < teacher_forcing_ratio\n",
    "            top1 = output.argmax(1) \n",
    "            x = eng_indices[t] if teacher_force else top1\n",
    "            \n",
    "        return outputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ded3fc18",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from torch.amp import autocast, GradScaler\n",
    "\n",
    "def train_fn(model, optimizer, loss_fn, clip, dataloader, device, scaler):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for esp_src, eng_trg in tqdm(dataloader, desc=\"Training\", leave=False):\n",
    "        esp_src = esp_src.transpose(0, 1).to(device)\n",
    "        eng_trg = eng_trg.transpose(0, 1).to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        with autocast('cuda'):\n",
    "            outputs = model(esp_src, eng_trg)\n",
    "            output_dim = outputs.shape[-1]\n",
    "            outputs_reshaped = outputs[:-1].reshape(-1, output_dim)        \n",
    "            trg_reshaped = eng_trg[1:].reshape(-1)                              # outputs[:-1] and eng_trg[1:} as an offset is created between outputs and eng_trg; outputs[i] is a prediction for eng_trg[i+1]\n",
    "            loss = loss_fn(outputs_reshaped, trg_reshaped)\n",
    "        \n",
    "        scaler.scale(loss).backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)                # to prevent exploding grads\n",
    "        scaler.step(optimizer) \n",
    "        scaler.update()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "    \n",
    "    return total_loss/len(dataloader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4ab409b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_fn(model, dataloader, loss_fn, device):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for src, trg in dataloader:\n",
    "            src = src.to(device)\n",
    "            trg = trg.to(device)\n",
    "            \n",
    "            outputs = model(src, trg, 0)            # Turn off teacher forcing for evaluation\n",
    "            \n",
    "            output_dim = outputs.shape[-1]\n",
    "            outputs_reshaped = outputs[:-1].view(-1, output_dim)\n",
    "            trg_reshaped = trg[1:].view(-1)\n",
    "            \n",
    "            loss = loss_fn(outputs_reshaped, trg_reshaped)\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            \n",
    "    return total_loss / len(dataloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a106883",
   "metadata": {},
   "source": [
    "### Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "826fce9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "N_EPOCHS = 100\n",
    "CLIP = 1\n",
    "LEARNING_RATE = 0.003\n",
    "WEIGHT_DECAY =  1e-5\n",
    "esp_vocab_size = len(word2idx_esp)\n",
    "eng_vocab_size = len(word2idx_eng)\n",
    "batch_size = 32\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc4cfb9f",
   "metadata": {},
   "source": [
    "### Final Data prep before training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "840acab8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "src_tensor = torch.LongTensor(esp_tensor)\n",
    "trg_tensor = torch.LongTensor(eng_tensor)\n",
    "\n",
    "train_src_tensor, valid_src_tensor, train_trg_tensor, valid_trg_tensor = train_test_split(\n",
    "    src_tensor.transpose(0, 1),  # Shape [20000, 57]\n",
    "    trg_tensor.transpose(0, 1),  # Shape [20000, 57]\n",
    "    test_size=0.1,\n",
    "    random_state=42\n",
    ")\n",
    "train_dataset = TensorDataset(train_src_tensor, train_trg_tensor)\n",
    "valid_dataset = TensorDataset(valid_src_tensor, valid_trg_tensor)\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "    train_dataset, \n",
    "    batch_size=batch_size, \n",
    "    shuffle=True, \n",
    "    num_workers=4,\n",
    "    pin_memory=True\n",
    ")\n",
    "\n",
    "valid_dataloader = DataLoader(\n",
    "    valid_dataset, \n",
    "    batch_size=batch_size, \n",
    "    num_workers=4, \n",
    "    pin_memory=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b61e180b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([57, 40000])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "src_tensor.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "445e49a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def epoch_time(start_time, end_time):\n",
    "    # This calculates the total duration in seconds. \n",
    "    elapsed_time = end_time - start_time\n",
    "    \n",
    "    # This converts the total seconds to whole minutes by dividing by 60 \n",
    "    elapsed_mins = int(elapsed_time / 60)\n",
    "    \n",
    "    # This calculates the remaining seconds. \n",
    "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
    "    \n",
    "    # It returns the calculated minutes and seconds.\n",
    "    return elapsed_mins, elapsed_secs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce9503d8",
   "metadata": {},
   "source": [
    "### Model Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "850fc84e",
   "metadata": {},
   "outputs": [],
   "source": [
    "enc = Encoder(esp_vocab_size, 256, 512, 2, 0.5)\n",
    "attn = Attention(512)\n",
    "dec = Decoder(eng_vocab_size, 256, 512, 2, 0.5, attn)\n",
    "model = translator(enc, dec, PAD_IDX_ESP, device).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bacdd888",
   "metadata": {},
   "source": [
    "# Training and Evaluating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f78a4fe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "scalar = GradScaler('cuda')\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=PAD_IDX_ENG)\n",
    "\n",
    "for epoch in range(N_EPOCHS):\n",
    "        start_time = time.time()\n",
    "        \n",
    "        train_loss = train_fn(model, optimizer, criterion, CLIP, train_dataloader, device,scalar)\n",
    "        valid_loss = evaluate_fn(model, valid_dataloader, criterion, device)\n",
    "        \n",
    "        end_time = time.time()\n",
    "        \n",
    "        epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "        \n",
    "        \n",
    "        if epoch%5 == 0:\n",
    "            torch.save(model.state_dict(), 'model.pt')\n",
    "        \n",
    "        print(f'Epoch: {epoch+1:02} | Time: {epoch_mins}m {epoch_secs}s')\n",
    "        print(f'\\tTrain Loss: {train_loss:.3f} | Train PPL: {math.exp(train_loss):7.3f}')\n",
    "        print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. PPL: {math.exp(valid_loss):7.3f}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "805eb18b",
   "metadata": {},
   "source": [
    "##### Note: i tried tuning the model several times but could not get the lost to decrease on the long run. Below is the example that I had saved for the best trained model, but lost the weights;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ad00233",
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate_sentence(sentence, src_vocab, trg_vocab, model, device, max_len=MAX_LENGTH):\n",
    "    model.eval()\n",
    "    processed_sentence = process_text(sentence)\n",
    "    token2idx_ex = []\n",
    "    for word in processed_sentence.split():\n",
    "        token2idx_ex.append(word2idx_esp.get(word, unk_idx_esp))\n",
    "    print(token2idx_ex)\n",
    "    seq = []\n",
    "    if len(token2idx_ex) > max_len:\n",
    "        seq = token2idx_ex[:max_len]             #Clip the sequence\n",
    "    else:\n",
    "        seq = token2idx_ex + [0] * (max_len - len(seq))       # Pad the sequence\n",
    "    ex_tensor = torch.tensor(seq, dtype=torch.long).to(device).unsqueeze(1)\n",
    "    with torch.no_grad():\n",
    "        encoder_outputs, hidden, cell = model.encoder(ex_tensor)\n",
    "        mask = model.create_mask(ex_tensor)\n",
    "\n",
    "    trg_indices = [trg_vocab['<sos>']]\n",
    "    attentions = torch.zeros(max_len, 1, len(ex_tensor)).to(device)\n",
    "    \n",
    "    for i in range(max_len):\n",
    "        trg_tensor = torch.LongTensor([trg_indices[-1]]).to(device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            output, hidden, cell, attention = model.decoder(trg_tensor, hidden, cell, encoder_outputs, mask)\n",
    "        \n",
    "        attentions[i] = attention\n",
    "        pred_token = output.argmax(1).item()\n",
    "        trg_indices.append(pred_token)\n",
    "\n",
    "        if pred_token == trg_vocab['<eos>']:\n",
    "            break\n",
    "            \n",
    "    idx2word_trg = {idx: word for word, idx in trg_vocab.items()}\n",
    "    trg_tokens = [idx2word_trg.get(i, '<unk>') for i in trg_indices]\n",
    "    \n",
    "    return trg_tokens[1:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3618039",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2, 347, 3]\n",
      "Source: haber\n",
      "Predicted Translation: i have\n"
     ]
    }
   ],
   "source": [
    "example_sentence = \"haber\"\n",
    "translated_tokens = translate_sentence(\n",
    "        example_sentence, word2idx_esp, word2idx_eng, model, device\n",
    "    )\n",
    "    \n",
    "print(f'Source: {example_sentence}')\n",
    "print(f'Predicted Translation: {\" \".join(translated_tokens)}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.13.3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
